{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pratyush\\AppData\\Local\\Temp\\ipykernel_10308\\70640425.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from polar import PolarCode,get_frozen\n",
    "from reliability_sequence import Reliability_Sequence\n",
    "from utils import errors_ber,errors_bler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "N = 2**n \n",
    "K = 9\n",
    "snr = 1\n",
    "batch_size = 1\n",
    "num_samples = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data(num_samples, batch_size, n, K, snr):\n",
    "    msg_bits_list = []\n",
    "    bpsk_list = []\n",
    "    codeword_list = []\n",
    "    corrupted_codeword_list = []\n",
    "\n",
    "    # Loop to generate data samples\n",
    "    for i in range(num_samples):\n",
    "        msg_bits = (torch.rand(batch_size, K) > 0.5).float()\n",
    "        bpsk = 1 - 2 * msg_bits\n",
    "\n",
    "        polar = PolarCode(n, K, Fr=None, use_cuda=True, hard_decision=True)\n",
    "        codeword = polar.encode(bpsk)\n",
    "        corrupted_codewords = polar.channel(codeword, snr)\n",
    "\n",
    "        msg_bits_list.append(msg_bits.cpu().numpy())\n",
    "        bpsk_list.append(bpsk.cpu().numpy())\n",
    "        codeword_list.append(codeword.cpu().numpy())\n",
    "        corrupted_codeword_list.append(corrupted_codewords.cpu().numpy())\n",
    "\n",
    "    filename = f\"polar_dataset_N{2**n}_K{K}_SNR{snr}_bs{batch_size}.npz\"\n",
    "    np.savez(f\"polar_dataset_N{2**n}_K{K}_SNR{snr}_bs{batch_size}.npz\", msg_bits=msg_bits_list, corrupted_codeword=corrupted_codeword_list,bpsk = bpsk_list,codeword=codeword_list)\n",
    "    print(f\"Dataset saved as {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as polar_dataset_N8_K3_SNR1_bs1.npz\n"
     ]
    }
   ],
   "source": [
    "create_data(num_samples,batch_size,n,K,snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('data\\polar_dataset_N16_K9_SNR1_bs1.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 68.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "Fr = get_frozen(N, K,rs=Reliability_Sequence)\n",
    "polar = PolarCode(n, K, Fr = Fr,use_cuda=False,hard_decision=True)\n",
    "device = 'cpu'\n",
    "ber_SC_total=0\n",
    "bler_SC_total=0\n",
    "x=10000\n",
    "for bpsk_bits, corrupted_codeword in tqdm(zip(df['bpsk'][:x], df['corrupted_codeword'][:x]),total=len(df['bpsk'][:x])):\n",
    "    bpsk_tensor = torch.tensor(bpsk_bits, dtype=torch.float32,device=device)\n",
    "    corrupted_codeword_tensor = torch.tensor(corrupted_codeword, dtype=torch.float32,device=device)\n",
    "\n",
    "    SC_llrs, decoded_SC_msg_bits = polar.sc_decode_new(corrupted_codeword_tensor, snr=snr)\n",
    "    ber_SC = errors_ber(bpsk_tensor,decoded_SC_msg_bits.sign()).item()\n",
    "    bler_SC = errors_bler(bpsk_tensor,decoded_SC_msg_bits.sign()).item()\n",
    "\n",
    "    ber_SC_total+=ber_SC\n",
    "    bler_SC_total+=bler_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.]]),\n",
       " tensor([[ 2.4066,  0.3885,  1.5644,  1.8708,  2.0070,  0.9793,  1.5226,  2.1272,\n",
       "           0.1533,  1.7197, -0.5839, -0.1718, -0.6325, -1.8706, -0.7579, -2.6515]]),\n",
       " tensor([[-1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_SC_msg_bits, corrupted_codeword_tensor, bpsk_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2923333366438746, 0.815)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ber_SC_total/len(df['msg_bits'][:x]),bler_SC_total/len(df['msg_bits'][:x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM LOGIC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self,N,K,hidden_size,rs):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.frozen_positions = get_frozen(self.N, self.K, rs)\n",
    "        self.frozen_mask = torch.full((N,), -1, dtype=torch.int8)\n",
    "        # self.frozen_mask = torch.zeros(N,dtype=torch.bool)\n",
    "        self.frozen_mask[self.frozen_positions] = 1\n",
    "        self.lstms = nn.ModuleList([\n",
    "            nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "            for _ in range(N)\n",
    "        ])\n",
    "        self.fcs= nn.ModuleList([\n",
    "            nn.Linear(hidden_size, 1) for _ in range(N)\n",
    "        ])\n",
    "    \n",
    "    def forward(self,corrupted_codeword):\n",
    "        batch_size = corrupted_codeword.size(0)\n",
    "        device = corrupted_codeword.device\n",
    "        x = corrupted_codeword.unsqueeze(-1) if corrupted_codeword.dim() == 2 else corrupted_codeword\n",
    "\n",
    "        h0 = [torch.zeros(1, batch_size, self.hidden_size, device=device) for _ in range(self.N)]\n",
    "        c0 = [torch.zeros(1, batch_size, self.hidden_size, device=device) for _ in range(self.N)]\n",
    "\n",
    "        decoded_outputs = []\n",
    "        for i in range(self.N):\n",
    "            output, (h0[i], c0[i]) = self.lstms[i](x, (h0[i], c0[i]))\n",
    "            print(output.shape)\n",
    "            decoded_bits = self.fcs[i](output).squeeze(-1)\n",
    "            decoded_outputs.append(decoded_bits)\n",
    "            x = corrupted_codeword + (self.frozen_mask.float().to(device)*decoded_bits.sign()) #current logic\n",
    "            x = x.unsqueeze(-1)\n",
    "        \n",
    "        decoded_outputs = torch.stack(decoded_outputs,dim=1)\n",
    "        # non_frozen_mask = ~self.frozen_mask\n",
    "        non_frozen_mask = (self.frozen_mask == -1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        final_predictions = decoded_outputs[:,-1,non_frozen_mask]\n",
    "\n",
    "        return decoded_outputs, final_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM LOGIC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder2(nn.Module):\n",
    "    def __init__(self,N,K,hidden_size,rs):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.frozen_positions = get_frozen(self.N, self.K, rs)\n",
    "        self.frozen_mask = torch.full((N,), 0, dtype=torch.int8)\n",
    "        self.frozen_mask[self.frozen_positions] = 1\n",
    "        \n",
    "        self.lstms = nn.ModuleList([\n",
    "            nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "            for _ in range(N)\n",
    "        ])\n",
    "        self.fcs= nn.ModuleList([\n",
    "            nn.Linear(hidden_size+1, 1) for _ in range(N) # 1 for the frozen bit\n",
    "        ])\n",
    "    \n",
    "    def forward(self,corrupted_codeword):\n",
    "        batch_size = corrupted_codeword.size(0)\n",
    "        device = corrupted_codeword.device\n",
    "        x = corrupted_codeword.unsqueeze(-1) if corrupted_codeword.dim() == 2 else corrupted_codeword\n",
    "\n",
    "        h0 = [torch.zeros(1, batch_size, self.hidden_size, device=device) for _ in range(self.N)]\n",
    "        c0 = [torch.zeros(1, batch_size, self.hidden_size, device=device) for _ in range(self.N)]\n",
    "\n",
    "        decoded_outputs = []\n",
    "        for i in range(self.N):\n",
    "            x_t = x[:,i,:].unsqueeze(1)\n",
    "            lstm_output, (h0[i], c0[i]) = self.lstms[i](x_t, (h0[i], c0[i]))\n",
    "        \n",
    "            frozen_bit = self.frozen_mask[i].float().to(device) #single predicted bit\n",
    "            frozen_bit = frozen_bit.unsqueeze(0).expand(batch_size, 1, 1)  #[B,N, prediction_value]          \n",
    "            lstm_output_with_frozen = torch.cat([lstm_output, frozen_bit], dim=-1) #[B,N,H+1]\n",
    "            predicted_bit = self.fcs[i](lstm_output_with_frozen).squeeze(-1) #[B,1]\n",
    "            decoded_outputs.append(predicted_bit)\n",
    "            # x = corrupted_codeword + (self.frozen_mask.float().to(device)*decoded_bits.sign()) #current logic\n",
    "            # x = torch.cat([x, predicted_bit.unsqueeze(-1)], dim=2)[:,:,-1:]\n",
    "            x[:,i,:] = predicted_bit #x = [B,N,1]\n",
    "\n",
    "        \n",
    "        decoded_outputs = torch.stack(decoded_outputs,dim=1)\n",
    "        # non_frozen_mask = ~self.frozen_mask\n",
    "        non_frozen_mask = (self.frozen_mask == 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        final_predictions = decoded_outputs[:,-1,non_frozen_mask]\n",
    "\n",
    "        return decoded_outputs, final_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLSTMDecoder(nn.Module):\n",
    "    def __init__(self, N, K, hidden_size, rs):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.frozen_positions = get_frozen(self.N, self.K, rs)\n",
    "        self.frozen_mask = torch.full((N,), -1, dtype=torch.int8)\n",
    "        self.frozen_mask[self.frozen_positions] = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size * 2, num_heads=4)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.residual = nn.Linear(1, hidden_size * 2)\n",
    "\n",
    "    def forward(self, corrupted_codeword):\n",
    "        batch_size = corrupted_codeword.size(0)\n",
    "        device = corrupted_codeword.device\n",
    "        x = corrupted_codeword.unsqueeze(-1) if corrupted_codeword.dim() == 2 else corrupted_codeword\n",
    "\n",
    "        h0 = torch.zeros(2 * 2, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(2 * 2, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        attn_output = self.layer_norm(attn_output + lstm_out)\n",
    "\n",
    "        # Residual connection\n",
    "        residual = self.residual(x)\n",
    "        attn_output = attn_output + residual\n",
    "\n",
    "        decoded_bits = self.fc(attn_output).squeeze(-1)\n",
    "\n",
    "        # Handle frozen bits\n",
    "        decoded_bits[:, self.frozen_positions] = 0\n",
    "\n",
    "        non_frozen_mask = (self.frozen_mask == -1).nonzero(as_tuple=True)[0]\n",
    "        return decoded_bits[:, non_frozen_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "model = ImprovedLSTMDecoder(N=N, K=K, hidden_size=32, rs=Reliability_Sequence).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ber_bler(predictions, targets):\n",
    "    \"\"\" Calculate BER and BLER using errors_ber and errors_bler functions. \"\"\"\n",
    "    ber = errors_ber(targets, predictions.sign()).item()\n",
    "    bler = errors_bler(targets, predictions.sign()).item()\n",
    "    return ber, bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:06<00:00, 145.35it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 158.64it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 162.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Loss: 0.6446, BER: 0.4265, BLER: 0.9944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 162.74it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 157.34it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 161.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Loss: 0.6089, BER: 0.3828, BLER: 0.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 160.75it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 164.07it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 164.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Loss: 0.5932, BER: 0.3605, BLER: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 164.83it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 160.26it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 161.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Loss: 0.5759, BER: 0.3432, BLER: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 159.51it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 159.52it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 162.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Loss: 0.5377, BER: 0.3005, BLER: 0.9222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 158.65it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 160.89it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 162.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Loss: 0.4910, BER: 0.2607, BLER: 0.8733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 160.86it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 160.32it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 162.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Loss: 0.4293, BER: 0.2141, BLER: 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 161.82it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 163.01it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 164.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Loss: 0.3659, BER: 0.1684, BLER: 0.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 164.82it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 160.40it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 159.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Loss: 0.3264, BER: 0.1363, BLER: 0.6367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:05<00:00, 164.27it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 176.24it/s]\n",
      "100%|██████████| 900/900 [00:05<00:00, 165.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Loss: 0.2942, BER: 0.1165, BLER: 0.5889\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "patience = 5  # Early stopping patience\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "data_len = len(df['msg_bits'][:900])\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_ber, total_bler = 0, 0, 0\n",
    "\n",
    "    for msg_bits, corrupted_codeword in tqdm(zip(df['msg_bits'][:900], df['corrupted_codeword'][:900]), total=data_len):\n",
    "        msg_tensor = torch.tensor(msg_bits, dtype=torch.float32).to(device)\n",
    "        corrupted_codeword_tensor = torch.tensor(corrupted_codeword, dtype=torch.float32).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        final_predictions = model(corrupted_codeword_tensor)\n",
    "        # print(final_predictions.shape)\n",
    "        loss = criterion(final_predictions, msg_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        ber, bler = calculate_ber_bler((final_predictions >= 0).float(), msg_tensor)\n",
    "        total_ber += ber\n",
    "        total_bler += bler\n",
    "\n",
    "    # Compute average loss and error rates\n",
    "    avg_loss = total_loss / data_len\n",
    "    avg_ber = total_ber / data_len\n",
    "    avg_bler = total_bler / data_len\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Logging progress every 3 epochs\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, BER: {avg_ber:.4f}, BLER: {avg_bler:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions tensor([[1., 1., 1., 0., 0., 0., 1., 1., 0.]], device='cuda:0')\n",
      "msg_tensor tensor([[1., 0., 1., 0., 1., 0., 1., 1., 0.]], device='cuda:0')\n",
      "decoded incorrectly\n",
      "----------\n",
      "final_predictions tensor([[0., 1., 0., 1., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "msg_tensor tensor([[0., 1., 1., 1., 1., 1., 1., 0., 0.]], device='cuda:0')\n",
      "decoded incorrectly\n",
      "----------\n",
      "final_predictions tensor([[0., 0., 0., 1., 0., 1., 1., 0., 1.]], device='cuda:0')\n",
      "msg_tensor tensor([[0., 0., 0., 1., 0., 0., 1., 0., 1.]], device='cuda:0')\n",
      "decoded incorrectly\n",
      "----------\n",
      "final_predictions tensor([[1., 0., 0., 0., 1., 1., 0., 1., 1.]], device='cuda:0')\n",
      "msg_tensor tensor([[1., 1., 0., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n",
      "decoded incorrectly\n",
      "----------\n",
      "final_predictions tensor([[1., 1., 1., 1., 0., 0., 0., 0., 1.]], device='cuda:0')\n",
      "msg_tensor tensor([[1., 1., 1., 0., 0., 0., 0., 0., 1.]], device='cuda:0')\n",
      "decoded incorrectly\n",
      "----------\n",
      "Test Results - BER: 0.3711, BLER: 1.0000\n"
     ]
    }
   ],
   "source": [
    "ber_total,bler_total = 0,0\n",
    "test_loader = df['msg_bits'][900:]\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for msg_bits, corrupted_codeword in zip(df['msg_bits'][900:], df['corrupted_codeword'][900:]):\n",
    "        count += 1\n",
    "        msg_tensor = torch.tensor(msg_bits, dtype=torch.float32).to(device)\n",
    "        corrupted_codeword_tensor = torch.tensor(corrupted_codeword, dtype=torch.float32).to(device)\n",
    "\n",
    "        final_predictions = model(corrupted_codeword_tensor)\n",
    "        if (count%20==0):\n",
    "            print(f'final_predictions {(final_predictions>=0).float()}')\n",
    "            print(f'msg_tensor {msg_tensor}')\n",
    "            if((final_predictions>0).float()==msg_tensor).all():\n",
    "                print('decoded correctly')\n",
    "            else:\n",
    "                print('decoded incorrectly')\n",
    "            print('----------')\n",
    "        ber,bler = calculate_ber_bler((final_predictions>=0).float(),msg_tensor)\n",
    "        ber_total += ber\n",
    "        bler_total += bler\n",
    "    avg_ber = ber_total / len(test_loader)\n",
    "    avg_bler = bler_total / len(test_loader)\n",
    "    print(f\"Test Results - BER: {avg_ber:.4f}, BLER: {avg_bler:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
